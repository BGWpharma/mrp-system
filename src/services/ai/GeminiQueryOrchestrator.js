// src/services/ai/GeminiQueryOrchestrator.js

import { DATABASE_TOOLS } from './tools/databaseTools.js';
import { ToolExecutor } from './tools/toolExecutor.js';

/**
 * Orchestrator zapyta≈Ñ AI u≈ºywajƒÖcy Google Gemini 2.5 Pro
 * 
 * Funkcje:
 * - Function Calling (podobnie jak OpenAI)
 * - Thinking Mode (rozumowanie przed odpowiedziƒÖ)
 * - 1M token√≥w kontekstu
 * - Inteligentny wyb√≥r modelu
 * 
 * Modele:
 * - gemini-2.5-pro (g≈Ç√≥wny - thinking, 1M tokens)
 * - gemini-1.5-pro (fallback - 2M tokens)
 * - gemini-2.0-flash-exp (szybki - 1M tokens, darmowy)
 */
export class GeminiQueryOrchestrator {
  
  /**
   * Konwertuje OpenAI tools format na Gemini function declarations
   */
  static convertToolsToGeminiFormat(tools) {
    return tools.map(tool => ({
      name: tool.function.name,
      description: tool.function.description,
      parameters: tool.function.parameters
    }));
  }
  
  /**
   * Inteligentny wyb√≥r modelu na podstawie zapytania
   */
  static selectBestModel(query, options = {}) {
    // U≈ºytkownik mo≈ºe wymusiƒá konkretny model
    if (options.forceModel) {
      return {
        model: options.forceModel,
        enableThinking: options.enableThinking !== false,
        reason: 'Wymuszony model przez u≈ºytkownika'
      };
    }
    
    const needsReasoning = this.needsDeepReasoning(query);
    const needsMegaContext = this.needsMegaContext(query);
    const isSimple = this.isSimpleQuery(query);
    
    // Poziom 3: G≈Çƒôbokie rozumowanie (2.5 Pro + Thinking)
    if (needsReasoning) {
      return {
        model: 'gemini-2.5-pro',
        enableThinking: true,
        reason: 'üß† Z≈Ço≈ºona analiza - u≈ºywam 2.5 Pro z thinking mode'
      };
    }
    
    // Poziom 2: Mega kontekst (1.5 Pro - 2M tokens)
    if (needsMegaContext) {
      return {
        model: 'gemini-1.5-pro',
        enableThinking: false,
        reason: 'üìö Bardzo du≈ºy kontekst - u≈ºywam 1.5 Pro (2M token√≥w)'
      };
    }
    
    // Poziom 1: Szybkie zapytania (2.0 Flash - darmowy)
    if (isSimple && options.allowExperimental !== false) {
      return {
        model: 'gemini-2.0-flash-exp',
        enableThinking: false,
        reason: '‚ö° Proste zapytanie - u≈ºywam 2.0 Flash (szybki i darmowy)'
      };
    }
    
    // Domy≈õlny: 2.5 Pro (najlepszy balans)
    return {
      model: 'gemini-2.5-pro',
      enableThinking: options.enableThinking !== false,
      reason: '‚öôÔ∏è Standardowe zapytanie - u≈ºywam 2.5 Pro'
    };
  }
  
  /**
   * Sprawdza czy zapytanie wymaga g≈Çƒôbokiego rozumowania
   */
  static needsDeepReasoning(query) {
    const reasoningKeywords = [
      'optymalizuj', 'najlepszy', 'zoptymalizuj',
      'por√≥wnaj szczeg√≥≈Çowo', 'przeanalizuj dok≈Çadnie',
      'dlaczego', 'jak poprawiƒá', 'rekomenduj',
      'zaproponuj', 'co powinienem',
      'rentowno≈õƒá', 'mar≈ºa', 'zysk', 'oszczƒôdno≈õƒá',
      'strategia', 'plan dzia≈Çania'
    ];
    
    const lowerQuery = query.toLowerCase();
    return reasoningKeywords.some(kw => lowerQuery.includes(kw));
  }
  
  /**
   * Sprawdza czy potrzebny mega kontekst (>1M tokens)
   */
  static needsMegaContext(query) {
    const megaContextKeywords = ['wszystkie', 'ca≈Ço≈õƒá', 'kompletna'];
    const hasMultiple = (query.match(/\+/g) || []).length > 2;
    
    return megaContextKeywords.some(kw => query.toLowerCase().includes(kw)) && hasMultiple;
  }
  
  /**
   * Sprawdza czy to proste zapytanie
   */
  static isSimpleQuery(query) {
    const simplePatterns = [
      /^ile (jest|mamy)/i,
      /^poka≈º \d+ (MO|CO|receptur|zam√≥wie≈Ñ)/i,
      /^lista \d+/i,
      /^wy≈õwietl \d+/i
    ];
    return simplePatterns.some(pattern => pattern.test(query.trim()));
  }
  
  /**
   * G≈Ç√≥wna metoda przetwarzania zapytania
   */
  static async processQuery(query, apiKey, context = [], options = {}) {
    console.log('[GeminiQueryOrchestrator] üöÄ Rozpoczynam przetwarzanie zapytania:', query);
    
    const startTime = performance.now();
    const executedTools = [];
    let totalTokensUsed = 0;
    
    try {
      // Wybierz najlepszy model
      const modelSelection = this.selectBestModel(query, options);
      const { model, enableThinking, reason } = modelSelection;
      
      console.log(`[GeminiQueryOrchestrator] ${reason}`);
      console.log(`[GeminiQueryOrchestrator] üì± Model: ${model}`);
      
      // Przygotuj tools w formacie Gemini (opcjonalnie wy≈ÇƒÖczone dla zwyk≈Çej konwersacji)
      const disableTools = options.disableTools || false;
      const geminiTools = disableTools ? null : [{
        function_declarations: this.convertToolsToGeminiFormat(DATABASE_TOOLS)
      }];
      
      if (disableTools) {
        console.log('[GeminiQueryOrchestrator] üí¨ Tryb konwersacyjny - narzƒôdzia wy≈ÇƒÖczone');
      }
      
      // Przygotuj historiƒô konwersacji
      const history = context.map(msg => ({
        role: msg.role === 'assistant' ? 'model' : 'user',
        parts: [{ text: msg.content }]
      }));
      
      // System instruction (zmieniony dla trybu konwersacyjnego)
      const systemInstruction = {
        parts: [{ text: disableTools ? this.getConversationalSystemPrompt() : this.getSystemPrompt() }]
      };
      
      // Iteracyjne wywo≈Çywanie (max 5 rund dla tools, 1 runda dla konwersacji)
      const maxRounds = disableTools ? 1 : 5;
      let currentRound = 0;
      let finalResponse = null;
      
      while (currentRound < maxRounds) {
        currentRound++;
        
        console.log(`[GeminiQueryOrchestrator] üîÑ Runda ${currentRound}/${maxRounds}`);
        
        // Przygotuj request dla Gemini
        const requestBody = {
          contents: [
            ...history,
            {
              role: 'user',
              parts: [{ text: query }]
            }
          ],
          systemInstruction: systemInstruction,
          generationConfig: {
            temperature: 0.85,  // Zwiƒôkszone dla bardziej ekspansywnych odpowiedzi
            maxOutputTokens: model === 'gemini-2.5-pro' ? 65536 : 8192,
            topP: 0.95,  // Wiƒôksza r√≥≈ºnorodno≈õƒá w odpowiedziach
            topK: 64     // Wiƒôcej opcji s≈Ç√≥w do wyboru
          }
        };
        
        // Dodaj tools tylko je≈õli nie sƒÖ wy≈ÇƒÖczone
        if (geminiTools) {
          requestBody.tools = geminiTools;
        }
        
        // Gemini 2.5 Pro automatycznie u≈ºywa thinking mode - nie wymaga jawnej konfiguracji
        // API nie wspiera pola 'thinkingConfig' - thinking jest wbudowany w model
        if (model === 'gemini-2.5-pro') {
          console.log('[GeminiQueryOrchestrator] üß† Gemini 2.5 Pro (thinking mode wbudowany automatycznie)');
        }
        
        // Wywo≈Çaj Gemini API
        const response = await fetch(
          `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${apiKey}`,
          {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json'
            },
            body: JSON.stringify(requestBody)
          }
        );
        
        if (!response.ok) {
          const errorData = await response.json().catch(() => ({}));
          const errorMessage = errorData.error?.message || 'Unknown error';
          throw new Error(`Gemini API error: ${response.status} - ${errorMessage}`);
        }
        
        const data = await response.json();
        
        // Zlicz tokeny
        if (data.usageMetadata) {
          const tokensUsed = (data.usageMetadata.promptTokenCount || 0) + 
                            (data.usageMetadata.candidatesTokenCount || 0);
          totalTokensUsed += tokensUsed;
          console.log(`[GeminiQueryOrchestrator] üìä Tokeny: ${tokensUsed} (prompt: ${data.usageMetadata.promptTokenCount}, response: ${data.usageMetadata.candidatesTokenCount})`);
        }
        
        const candidate = data.candidates?.[0];
        if (!candidate) {
          throw new Error('Brak odpowiedzi od Gemini');
        }
        
        // Loguj finishReason dla debugowania
        console.log(`[GeminiQueryOrchestrator] üèÅ Finish reason: ${candidate.finishReason || 'unknown'}`);
        
        // Sprawd≈∫ czy odpowied≈∫ zosta≈Ça zablokowana
        if (candidate.finishReason === 'SAFETY' || candidate.finishReason === 'RECITATION') {
          throw new Error(`Odpowied≈∫ zosta≈Ça zablokowana: ${candidate.finishReason}`);
        }
        
        // Sprawd≈∫ czy osiƒÖgniƒôto limit token√≥w
        if (candidate.finishReason === 'MAX_TOKENS') {
          console.warn('[GeminiQueryOrchestrator] ‚ö†Ô∏è OsiƒÖgniƒôto limit token√≥w - odpowied≈∫ mo≈ºe byƒá niekompletna');
        }
        
        const content = candidate.content;
        
        // Sprawd≈∫ czy content istnieje
        if (!content) {
          console.error('[GeminiQueryOrchestrator] ‚ùå Brak content w odpowiedzi');
          console.error('[GeminiQueryOrchestrator] üìä Candidate:', JSON.stringify(candidate, null, 2));
          throw new Error(`Gemini zwr√≥ci≈Ç pustƒÖ odpowied≈∫. Finish reason: ${candidate.finishReason || 'unknown'}`);
        }
        
        const parts = content.parts || [];
        
        // Sprawd≈∫ czy sƒÖ function calls
        const functionCalls = parts.filter(part => part.functionCall);
        
        if (functionCalls.length > 0) {
          console.log(`[GeminiQueryOrchestrator] üîß Gemini wywo≈Çuje ${functionCalls.length} funkcji`);
          
          // Wykonaj wszystkie wywo≈Çania funkcji
          const functionResults = [];
          
          for (const call of functionCalls) {
            const functionName = call.functionCall.name;
            const functionArgs = call.functionCall.args || {};
            
            console.log(`[GeminiQueryOrchestrator] ‚öôÔ∏è Wykonujƒô: ${functionName}`, functionArgs);
            
            const toolResult = await ToolExecutor.executeFunction(functionName, functionArgs);
            
            executedTools.push({
              name: functionName,
              executionTime: toolResult.executionTime,
              success: toolResult.success
            });
            
            console.log(`[GeminiQueryOrchestrator] ‚úÖ ${functionName} wykonany w ${toolResult.executionTime.toFixed(2)}ms`);
            
            // Dodaj wynik w formacie Gemini
            functionResults.push({
              functionResponse: {
                name: functionName,
                response: toolResult.data
              }
            });
          }
          
          // Dodaj wyniki funkcji do historii
          history.push({
            role: 'user',
            parts: [{ text: query }]
          });
          
          history.push({
            role: 'model',
            parts: functionCalls
          });
          
          history.push({
            role: 'user',
            parts: functionResults
          });
          
          // Kontynuuj do nastƒôpnej rundy z wynikami
          continue;
        }
        
        // Je≈õli nie ma function calls, sprawd≈∫ czy jest tekstowa odpowied≈∫
        const textPart = parts.find(part => part.text);
        if (textPart) {
          finalResponse = textPart.text;
          console.log('[GeminiQueryOrchestrator] ‚úÖ Otrzymano finalnƒÖ odpowied≈∫');
          console.log('[GeminiQueryOrchestrator] üìù D≈Çugo≈õƒá:', finalResponse.length, 'znak√≥w');
          break;
        }
        
        // Je≈õli nic nie znaleziono, przerwij z ostrze≈ºeniem
        console.warn('[GeminiQueryOrchestrator] ‚ö†Ô∏è Brak function calls i brak tekstu - przerywam');
        console.warn('[GeminiQueryOrchestrator] üìä Parts w odpowiedzi:', parts.length);
        if (parts.length > 0) {
          console.warn('[GeminiQueryOrchestrator] üìä Typy parts:', parts.map(p => Object.keys(p).join(', ')));
        }
        break;
      }
      
      const processingTime = performance.now() - startTime;
      
      if (!finalResponse) {
        throw new Error('Nie otrzymano odpowiedzi od Gemini po wykonaniu funkcji');
      }
      
      console.log(`[GeminiQueryOrchestrator] üéâ Zako≈Ñczono w ${processingTime.toFixed(2)}ms`);
      console.log(`[GeminiQueryOrchestrator] üìä ≈ÅƒÖcznie token√≥w: ${totalTokensUsed}`);
      console.log(`[GeminiQueryOrchestrator] üîß Wykonano funkcji: ${executedTools.length}`);
      
      return {
        success: true,
        response: finalResponse,
        executedTools,
        tokensUsed: totalTokensUsed,
        processingTime,
        model: model
      };
      
    } catch (error) {
      console.error('[GeminiQueryOrchestrator] ‚ùå B≈ÇƒÖd:', error);
      return {
        success: false,
        error: error.message,
        executedTools,
        processingTime: performance.now() - startTime
      };
    }
  }
  
  /**
   * System prompt dla Gemini
   */
  static getSystemPrompt() {
    return `Jeste≈õ inteligentnym asystentem AI dla systemu MRP (Manufacturing Resource Planning).

Twoje zadanie: Analizujesz zapytania u≈ºytkownik√≥w i decydujesz jakie dane pobraƒá z bazy danych, u≈ºywajƒÖc dostƒôpnych funkcji.

Dostƒôpne funkcje (tools):
- query_recipes - receptury produkt√≥w
- query_inventory - stany magazynowe
- query_production_tasks - zadania produkcyjne (MO)
- query_orders - zam√≥wienia klient√≥w (CO)
- query_purchase_orders - zam√≥wienia zakupu (PO)
- query_inventory_transactions - transakcje magazynowe
- query_production_history - historia produkcji i produktywno≈õƒá
- get_system_alerts - alerty systemowe (niskie stany, wygasajƒÖce partie, op√≥≈∫nienia)
- calculate_production_costs - koszty produkcji i rentowno≈õƒá
- trace_material_flow - ≈õledzenie przep≈Çywu materia≈Ç√≥w (traceability)
- query_invoices - faktury
- query_cmr_documents - dokumenty CMR
- query_inventory_batches - partie magazynowe
- aggregate_data - agregacje (suma, ≈õrednia, min, max, grupowanie)
- get_count - szybkie zliczanie dokument√≥w
- get_customers, get_suppliers, get_users - dane kontrahent√≥w i u≈ºytkownik√≥w

PROCES PRACY:
1. Przeanalizuj zapytanie u≈ºytkownika
2. Zdecyduj kt√≥re funkcje wywo≈Çaƒá aby uzyskaƒá potrzebne dane
3. Wywo≈Çaj odpowiednie funkcje (mo≈ºesz wywo≈Çaƒá wiele naraz)
4. Przeanalizuj wyniki funkcji
5. Udziel konkretnej odpowiedzi w jƒôzyku polskim

WA≈ªNE ZASADY:
- U≈ºywaj konkretnych danych z wynik√≥w funkcji (nie wymy≈õlaj!)
- Formatuj odpowiedzi czytelnie (tabele, listy, punkty)
- Je≈õli brak danych, powiedz o tym jasno
- Dla z≈Ço≈ºonych analiz, rozumuj krok po kroku
- Zawsze odpowiadaj po polsku
- BƒÖd≈∫ profesjonalny i konkretny

FORMATOWANIE:
- U≈ºywaj tabel markdown dla por√≥wna≈Ñ (poka≈º WSZYSTKIE dostƒôpne kolumny)
- U≈ºywaj list dla wyliczenia
- U≈ºywaj emoji dla lepszej czytelno≈õci (ale z umiarem)
- Dodawaj podsumowania na ko≈Ñcu odpowiedzi

WA≈ªNE WARTO≈öCI (automatycznie normalizowane):
- Statusy zada≈Ñ produkcyjnych: mo≈ºesz u≈ºywaƒá "zaplanowane", "w trakcie", "wstrzymane", "zako≈Ñczone", "anulowane" (system automatycznie przekonwertuje na w≈Ça≈õciwe warto≈õci)
- Statusy zam√≥wie≈Ñ: mo≈ºesz u≈ºywaƒá "nowe", "w realizacji", "zako≈Ñczone", "anulowane", "wstrzymane"
- System automatycznie normalizuje wielko≈õƒá liter, wiƒôc mo≈ºesz pisaƒá ma≈Çymi literami

ZASADY SZCZEG√ì≈ÅOWO≈öCI:
‚≠ê Generuj PE≈ÅNE, SZCZEG√ì≈ÅOWE odpowiedzi - u≈ºytkownicy preferujƒÖ kompletne informacje
‚≠ê Pokazuj WSZYSTKIE dostƒôpne dane - je≈õli jest 10 rekord√≥w, poka≈º wszystkie 10
‚≠ê U≈ºywaj tabel z WIELOMA kolumnami, ≈ºeby pokazaƒá wiƒôcej szczeg√≥≈Ç√≥w
‚≠ê Dodawaj ANALIZY i INTERPRETACJE wynik√≥w, nie tylko surowe dane
‚≠ê Je≈õli zapytanie dotyczy analizy, bƒÖd≈∫ bardzo szczeg√≥≈Çowy i wyczerpujƒÖcy
‚≠ê Dla danych liczbowych: pokazuj sumy, ≈õrednie, trendy
‚≠ê Nie skracaj informacji - lepiej wiƒôcej ni≈º mniej

Jeste≈õ ekspertem w zarzƒÖdzaniu produkcjƒÖ i optymalizacji proces√≥w.`;
  }
  
  /**
   * System prompt dla trybu konwersacyjnego (bez dostƒôpu do bazy danych)
   */
  static getConversationalSystemPrompt() {
    return `Jeste≈õ pomocnym asystentem AI dla systemu MRP (Manufacturing Resource Planning).

Obecnie jeste≈õ w trybie konwersacyjnym - nie masz dostƒôpu do bazy danych, ale mo≈ºesz:
- Odpowiadaƒá na og√≥lne pytania o system MRP
- Udzielaƒá porad dotyczƒÖcych zarzƒÖdzania produkcjƒÖ
- Wyja≈õniaƒá pojƒôcia i koncepcje
- Prowadziƒá przyjaznƒÖ rozmowƒô
- Pomagaƒá zrozumieƒá funkcje systemu

ZASADY:
- Zawsze odpowiadaj po polsku
- BƒÖd≈∫ pomocny, przyjazny i profesjonalny
- Je≈õli u≈ºytkownik chce konkretne dane z systemu, poinformuj go, ≈ºe mo≈ºe zadaƒá konkretne pytanie o dane (np. "Poka≈º ostatnie MO", "Ile mamy receptur?")
- U≈ºywaj emoji dla lepszej czytelno≈õci, ale z umiarem
- Formatuj odpowiedzi czytelnie (u≈ºywaj list, nag≈Ç√≥wk√≥w, podzia≈Ç√≥w)

Pamiƒôtaj: Jeste≈õ ekspertem w zarzƒÖdzaniu produkcjƒÖ i mo≈ºna Ciƒô pytaƒá o wszystko! üí¨`;
  }
  
  /**
   * Sprawdza czy zapytanie powinno byƒá obs≈Çu≈ºone przez orchestrator
   */
  static shouldHandle(query) {
    const dataKeywords = [
      // Czasowniki akcji
      'ile', 'poka≈º', 'wy≈õwietl', 'lista', 'jaki', 'jakie', 'kt√≥ry', 'kt√≥re',
      'podaj', 'daj', 'znajd≈∫', 'szukaj', 'pobierz', 'sprawd≈∫', 'zobacz',
      
      // Rzeczowniki i obszary
      'receptur', 'magazyn', 'produkcj', 'zam√≥wi', 'mo', 'co', 'po',
      'klient', 'dostawc', 'faktur', 'cmr', 'stan', 'alert', 'koszt',
      'u≈ºytkownik', 'pracownik', 'wydajno≈õƒá', 'produktywn', 'transakcj',
      'partii', 'partie', 'wygasa', 'op√≥≈∫nion', 'uwag', 'problem',
      'rentowno≈õƒá', 'mar≈ºa', 'zysk', 'analiz', 'por√≥wna', 'optymalizuj',
      
      // Dodatkowe s≈Çowa kluczowe
      'historia', 'sesj', 'raport', 'statystyk', 'zu≈ºyc', 'rezerwacj'
    ];
    
    const lowerQuery = query.toLowerCase();
    return dataKeywords.some(keyword => lowerQuery.includes(keyword));
  }
  
  /**
   * Szacuje koszt zapytania (Gemini pricing)
   */
  static estimateCost(tokensUsed, model = 'gemini-2.5-pro') {
    // Pricing (per 1M tokens)
    const pricing = {
      'gemini-2.5-pro': { input: 1.25, output: 5.00 },
      'gemini-1.5-pro': { input: 1.25, output: 5.00 },
      'gemini-2.0-flash-exp': { input: 0, output: 0 }, // Darmowy w exp
      'gemini-1.5-flash': { input: 0.075, output: 0.30 }
    };
    
    const modelPricing = pricing[model] || pricing['gemini-2.5-pro'];
    
    // Zak≈Çadamy 50/50 input/output
    const inputCost = (tokensUsed * 0.5) * (modelPricing.input / 1000000);
    const outputCost = (tokensUsed * 0.5) * (modelPricing.output / 1000000);
    
    return inputCost + outputCost;
  }
}

