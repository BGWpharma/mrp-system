// src/services/ai/AIQueryOrchestrator.js

import { DATABASE_TOOLS } from './tools/databaseTools.js';
import { ToolExecutor } from './tools/toolExecutor.js';

/**
 * Orchestrator zapyta≈Ñ AI - u≈ºywa GPT do decydowania jakie zapytania wykonaƒá do bazy
 * 
 * System dzia≈Ça w nastƒôpujƒÖcy spos√≥b:
 * 1. U≈ºytkownik zadaje pytanie w jƒôzyku naturalnym
 * 2. GPT analizuje pytanie i decyduje jakie funkcje (tools) wywo≈Çaƒá
 * 3. System wykonuje wybrane funkcje (targetowane zapytania do Firestore)
 * 4. GPT otrzymuje wyniki i generuje odpowied≈∫ w jƒôzyku naturalnym
 * 
 * Zalety:
 * - Pobiera TYLKO potrzebne dane (nie ca≈ÇƒÖ bazƒô)
 * - Elastyczny - dzia≈Ça z dowolnymi zapytaniami
 * - AI sam orkiestruje zapytania
 * - Mo≈ºliwo≈õƒá wielu rund (AI mo≈ºe wywo≈Çywaƒá funkcje sekwencyjnie)
 */
export class AIQueryOrchestrator {
  
  /**
   * Przetwarza zapytanie u≈ºywajƒÖc AI do decydowania o zapytaniach do bazy
   * @param {string} query - Zapytanie u≈ºytkownika
   * @param {string} apiKey - Klucz API OpenAI
   * @param {Array} context - Kontekst konwersacji (poprzednie wiadomo≈õci)
   * @param {Object} options - Opcje dodatkowe
   * @returns {Promise<Object>} - Wynik przetwarzania
   */
  static async processQuery(query, apiKey, context = [], options = {}) {
    console.log('[AIQueryOrchestrator] üöÄ Rozpoczynam przetwarzanie zapytania:', query);
    
    const startTime = performance.now();
    const executedTools = [];
    let totalTokensUsed = 0;
    
    try {
      // Krok 1: Przygotuj wiadomo≈õci dla GPT
      let messages = [
        {
          role: 'system',
          content: this.getSystemPrompt()
        },
        ...context.map(msg => ({
          role: msg.role,
          content: msg.content
        })),
        {
          role: 'user',
          content: query
        }
      ];
      
      // Iteracyjne wywo≈Çywanie API (maksymalnie 5 rund - AI mo≈ºe wywo≈Çywaƒá funkcje wielokrotnie)
      const maxRounds = 5;
      let currentRound = 0;
      let finalResponse = null;
      
      while (currentRound < maxRounds) {
        currentRound++;
        
        console.log(`[AIQueryOrchestrator] üîÑ Runda ${currentRound}/${maxRounds}: Wysy≈Çam do GPT...`);
        
        // Krok 2: Wywo≈Çanie OpenAI API z tools
        const requestBody = {
          model: options.model || 'gpt-4o',  // lub gpt-4o-mini dla oszczƒôdno≈õci
          messages: messages,
          tools: DATABASE_TOOLS,
          tool_choice: 'auto',  // GPT sam decyduje czy wywo≈Çaƒá narzƒôdzia
          temperature: 0.7,
          max_tokens: 2000
        };
        
        // UWAGA: OpenAI nie obs≈Çuguje streamingu z Function Calling (tools)
        // Je≈õli onChunk jest przekazany, wy≈ÇƒÖczamy tools - ale to nie powinno siƒô zdarzyƒá
        // bo w aiAssistantService.js nie przekazujemy onChunk do orchestratora
        if (options.onChunk) {
          console.warn('[AIQueryOrchestrator] ‚ö†Ô∏è Otrzymano callback streamingu - ale orchestrator nie obs≈Çuguje streamingu z tools!');
          console.warn('[AIQueryOrchestrator] üí° Streaming zostanie zignorowany, aby umo≈ºliwiƒá Function Calling');
          // NIE usuwamy tools - Function Calling jest priorytetem
          // delete requestBody.tools;
          // delete requestBody.tool_choice;
        }
        
        const response = await fetch('https://api.openai.com/v1/chat/completions', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${apiKey}`
          },
          body: JSON.stringify(requestBody)
        });
        
        if (!response.ok) {
          const errorData = await response.json().catch(() => ({}));
          throw new Error(`OpenAI API error: ${response.status} ${response.statusText} - ${errorData.error?.message || 'Unknown error'}`);
        }
        
        const data = await response.json();
        const assistantMessage = data.choices[0].message;
        
        // ≈öled≈∫ u≈ºycie token√≥w
        if (data.usage) {
          totalTokensUsed += data.usage.total_tokens;
          console.log(`[AIQueryOrchestrator] üìä U≈ºyto ${data.usage.total_tokens} token√≥w (${data.usage.prompt_tokens} prompt + ${data.usage.completion_tokens} completion)`);
        }
        
        // Dodaj odpowied≈∫ asystenta do kontekstu
        messages.push(assistantMessage);
        
        // Krok 3: Sprawd≈∫ czy GPT chce wywo≈Çaƒá narzƒôdzia
        if (assistantMessage.tool_calls && assistantMessage.tool_calls.length > 0) {
          console.log(`[AIQueryOrchestrator] üîß GPT chce wywo≈Çaƒá ${assistantMessage.tool_calls.length} narzƒôdzi:`);
          
          assistantMessage.tool_calls.forEach(tc => {
            console.log(`  - ${tc.function.name}`);
          });
          
          // Wykonaj wszystkie wywo≈Çania narzƒôdzi r√≥wnolegle
          const toolResults = await Promise.all(
            assistantMessage.tool_calls.map(async (toolCall) => {
              const functionName = toolCall.function.name;
              let functionArgs;
              
              try {
                functionArgs = JSON.parse(toolCall.function.arguments);
              } catch (parseError) {
                console.error(`[AIQueryOrchestrator] ‚ùå B≈ÇƒÖd parsowania argument√≥w dla ${functionName}:`, parseError);
                return {
                  tool_call_id: toolCall.id,
                  role: 'tool',
                  name: functionName,
                  content: JSON.stringify({ 
                    error: 'B≈ÇƒÖd parsowania argument√≥w',
                    details: parseError.message
                  })
                };
              }
              
              console.log(`[AIQueryOrchestrator] ‚öôÔ∏è Wykonujƒô: ${functionName}`, functionArgs);
              
              const result = await ToolExecutor.executeFunction(functionName, functionArgs);
              
              if (result.success) {
                executedTools.push({
                  name: functionName,
                  arguments: functionArgs,
                  result: result.data,
                  executionTime: result.executionTime
                });
                
                console.log(`[AIQueryOrchestrator] ‚úÖ ${functionName} wykonany w ${result.executionTime.toFixed(2)}ms`);
              } else {
                console.error(`[AIQueryOrchestrator] ‚ùå ${functionName} zako≈Ñczy≈Ç siƒô b≈Çƒôdem:`, result.error);
              }
              
              // Zwr√≥ƒá wynik do GPT
              return {
                tool_call_id: toolCall.id,
                role: 'tool',
                name: functionName,
                content: JSON.stringify(result.success ? result.data : { error: result.error })
              };
            })
          );
          
          // Dodaj wyniki narzƒôdzi do kontekstu
          messages.push(...toolResults);
          
          // Kontynuuj kolejnƒÖ rundƒô (GPT przetworzy wyniki i mo≈ºe wywo≈Çaƒá wiƒôcej funkcji lub wygenerowaƒá odpowied≈∫)
          continue;
        }
        
        // Krok 4: Je≈õli GPT nie wywo≈Ça≈Ç narzƒôdzi, mamy finalnƒÖ odpowied≈∫
        if (assistantMessage.content) {
          finalResponse = assistantMessage.content;
          console.log('[AIQueryOrchestrator] ‚úÖ Otrzymano finalnƒÖ odpowied≈∫ od GPT');
          console.log('[AIQueryOrchestrator] üìù Tre≈õƒá odpowiedzi:', finalResponse.substring(0, 200) + '...');
          console.log('[AIQueryOrchestrator] üìè D≈Çugo≈õƒá odpowiedzi:', finalResponse.length, 'znak√≥w');
          break;
        }
        
        // Je≈õli nie ma ani tool_calls ani content, co≈õ posz≈Ço nie tak
        if (!assistantMessage.content && !assistantMessage.tool_calls) {
          console.warn('[AIQueryOrchestrator] ‚ö†Ô∏è GPT nie zwr√≥ci≈Ç ani odpowiedzi ani wywo≈Ça≈Ñ narzƒôdzi');
          finalResponse = "Przepraszam, nie mog≈Çem przetworzyƒá tego zapytania. Spr√≥buj przeformu≈Çowaƒá pytanie.";
          break;
        }
      }
      
      // Je≈õli wyczerpali≈õmy rundy bez odpowiedzi
      if (!finalResponse && currentRound >= maxRounds) {
        console.warn('[AIQueryOrchestrator] ‚ö†Ô∏è OsiƒÖgniƒôto maksymalnƒÖ liczbƒô rund bez finalnej odpowiedzi');
        finalResponse = "Przepraszam, przetwarzanie tego zapytania wymaga zbyt wielu krok√≥w. Spr√≥buj upro≈õciƒá pytanie lub podzieliƒá je na mniejsze czƒô≈õci.";
      }
      
      const processingTime = performance.now() - startTime;
      
      console.log(`[AIQueryOrchestrator] üéâ Zako≈Ñczono w ${processingTime.toFixed(2)}ms`);
      console.log(`[AIQueryOrchestrator] üìä Statystyki:`);
      console.log(`  - Rundy: ${currentRound}`);
      console.log(`  - Wykonane funkcje: ${executedTools.length}`);
      console.log(`  - Tokeny u≈ºyte: ${totalTokensUsed}`);
      
      if (executedTools.length > 0) {
        console.log('[AIQueryOrchestrator] üìã Wykonane zapytania:');
        executedTools.forEach((tool, index) => {
          console.log(`  ${index + 1}. ${tool.name} (${tool.executionTime.toFixed(2)}ms)`);
        });
      }
      
      const result = {
        success: true,
        response: finalResponse,
        processingTime,
        executedTools,
        rounds: currentRound,
        tokensUsed: totalTokensUsed,
        method: 'ai_orchestrator',
        metadata: {
          dataSourcesQueried: [...new Set(executedTools.map(t => t.name))],
          totalQueries: executedTools.length,
          averageQueryTime: executedTools.length > 0 
            ? executedTools.reduce((sum, t) => sum + t.executionTime, 0) / executedTools.length 
            : 0
        }
      };
      
      console.log('[AIQueryOrchestrator] üéÅ Zwracam wynik:', {
        success: result.success,
        responseLength: result.response?.length,
        responsePreview: result.response?.substring(0, 100),
        executedTools: result.executedTools.length
      });
      
      return result;
      
    } catch (error) {
      console.error('[AIQueryOrchestrator] ‚ùå B≈ÇƒÖd krytyczny:', error);
      
      const processingTime = performance.now() - startTime;
      
      return {
        success: false,
        error: error.message,
        processingTime,
        executedTools,
        rounds: 0,
        method: 'ai_orchestrator_error'
      };
    }
  }
  
  /**
   * Generuje system prompt dla GPT
   */
  static getSystemPrompt() {
    return `Jeste≈õ zaawansowanym asystentem AI dla systemu MRP (Manufacturing Resource Planning) firmy BGW Pharma.

Twoje zadanie to:
1. Analizowanie zapyta≈Ñ u≈ºytkownika w jƒôzyku polskim
2. Wywo≈Çywanie odpowiednich funkcji do pobrania TYLKO potrzebnych danych z bazy
3. Generowanie czytelnych, pomocnych odpowiedzi po polsku

DOSTƒòPNE FUNKCJE (tools):
- query_recipes: receptury i przepisy
- query_inventory: stany magazynowe, partie, surowce
- query_production_tasks: zadania produkcyjne (MO)
- query_orders: zam√≥wienia klient√≥w (CO)
- query_purchase_orders: zam√≥wienia zakupu (PO)
- aggregate_data: agregacje (suma, ≈õrednia, grupowanie)
- get_count: szybkie zliczanie (najszybsze!)
- get_customers: lista klient√≥w
- get_suppliers: lista dostawc√≥w

WA≈ªNE ZASADY:
‚úÖ ZAWSZE u≈ºywaj filtr√≥w aby pobraƒá TYLKO potrzebne dane
‚úÖ Dla prostych pyta≈Ñ "ile jest..." u≈ºyj get_count (najszybsze)
‚úÖ Ogranicz limit do minimum (nie pobieraj 1000 rekord√≥w je≈õli wystarczy 10)
‚úÖ Je≈õli nie jeste≈õ pewien jakie dane potrzebne, wywo≈Çaj funkcjƒô z podstawowymi parametrami
‚úÖ Mo≈ºesz wywo≈Çywaƒá funkcje wielokrotnie je≈õli potrzebujesz wiƒôcej danych
‚úÖ Odpowiadaj ZAWSZE po polsku, u≈ºywajƒÖc polskiej terminologii

TERMINOLOGIA MRP:
- MO = Manufacturing Order = Zlecenie produkcyjne
- CO = Customer Order = Zam√≥wienie klienta  
- PO = Purchase Order = Zam√≥wienie zakupu
- Receptura = Przepis produkcyjny z listƒÖ sk≈Çadnik√≥w
- Partia = Batch = Konkretna dostawa materia≈Çu z numerem partii
- Stan magazynowy = Inventory = Aktualna ilo≈õƒá w magazynie

FORMATOWANIE ODPOWIEDZI:
- U≈ºywaj emoji dla lepszej czytelno≈õci (üìä üì¶ üè≠ ‚úÖ ‚ö†Ô∏è)
- Prezentuj dane w postaci list lub tabel
- Dodawaj podsumowania i insights
- Je≈õli dane pokazujƒÖ problem (niski stan, op√≥≈∫nienia), zasugeruj dzia≈Çanie

Przyk≈Çad dobrego workflow:
Zapytanie: "Ile mamy receptur o wadze ponad 900g?"
Krok 1: Wywo≈Çaj query_recipes({ calculateWeight: true, limit: 500 })
Krok 2: Przefiltruj wyniki i policz te > 900g
Krok 3: Wygeneruj odpowied≈∫ z listƒÖ receptur

Pamiƒôtaj: Twoim celem jest dostarczenie DOK≈ÅADNYCH danych w spos√≥b SZYBKI i EFEKTYWNY!`;
  }
  
  /**
   * Sprawdza czy orchestrator powinien obs≈Çu≈ºyƒá zapytanie
   * (u≈ºywane do decyzji czy u≈ºyƒá orchestratora czy standardowego systemu)
   */
  static shouldHandle(query) {
    // Orchestrator mo≈ºe obs≈Çu≈ºyƒá prawie wszystkie zapytania o dane
    // Jedyny wyjƒÖtek to zapytania wymagajƒÖce attachment√≥w (dokumenty, zdjƒôcia)
    
    const lowerQuery = query.toLowerCase();
    
    // Nie obs≈Çuguj zapyta≈Ñ o analizƒô za≈ÇƒÖcznik√≥w
    if (lowerQuery.includes('za≈ÇƒÖcznik') || 
        lowerQuery.includes('dokument') || 
        lowerQuery.includes('zdjƒôcie') ||
        lowerQuery.includes('plik')) {
      return false;
    }
    
    // Nie obs≈Çuguj zapyta≈Ñ stricte konwersacyjnych
    const conversationalPatterns = [
      /^(cze≈õƒá|hej|witaj|dzie≈Ñ dobry|dobry wiecz√≥r)/i,
      /^(dziƒôkujƒô|dziƒôki|thx|thanks)/i,
      /^(jak siƒô masz|co s≈Çychaƒá)/i
    ];
    
    if (conversationalPatterns.some(pattern => pattern.test(lowerQuery))) {
      return false;
    }
    
    // Obs≈Çuguj wszystkie inne zapytania
    return true;
  }
  
  /**
   * Szacuje koszt wykonania zapytania (w USD)
   */
  static estimateCost(tokensUsed, model = 'gpt-4o') {
    // Ceny OpenAI (stan na 2024)
    const pricing = {
      'gpt-4o': { input: 0.005, output: 0.015 },  // na 1K token√≥w
      'gpt-4o-mini': { input: 0.00015, output: 0.00060 }
    };
    
    const prices = pricing[model] || pricing['gpt-4o'];
    
    // Uproszczone - przyjmujemy 50/50 split input/output
    const avgPrice = (prices.input + prices.output) / 2;
    return (tokensUsed / 1000) * avgPrice;
  }
}

